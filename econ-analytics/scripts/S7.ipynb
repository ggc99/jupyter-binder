{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> Application of Support Vector Machine to Gene Expression Data (Khan.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0  c        V1        V2        V3        V4        V5        V6  \\\n",
      "0         V1  2  0.773344 -2.438405 -0.482562 -2.721135 -1.217058  0.827809   \n",
      "1         V2  2 -0.078178 -2.415754  0.412772 -2.825146 -0.626236  0.054488   \n",
      "2         V3  2 -0.084469 -1.649739 -0.241308 -2.875286 -0.889405 -0.027474   \n",
      "3         V4  2  0.965614 -2.380547  0.625297 -1.741256 -0.845366  0.949687   \n",
      "4         V5  2  0.075664 -1.728785  0.852626  0.272695 -1.841370  0.327936   \n",
      "\n",
      "         V7        V8  ...     V2299     V2300     V2301     V2302     V2303  \\\n",
      "0  1.342604  0.057042  ... -0.238511 -0.027474 -1.660205  0.588231 -0.463624   \n",
      "1  1.429498 -0.120249  ... -0.657394 -0.246284 -0.836325 -0.571284  0.034788   \n",
      "2  1.159300  0.015676  ... -0.696352  0.024985 -1.059872 -0.403767 -0.678653   \n",
      "3  1.093801  0.819736  ...  0.259746  0.357115 -1.893128  0.255107  0.163309   \n",
      "4  1.251219  0.771450  ... -0.200404  0.061753 -2.273998 -0.039365  0.368801   \n",
      "\n",
      "      V2304     V2305     V2306     V2307     V2308  \n",
      "0 -3.952845 -5.496768 -1.414282 -0.647600 -1.763172  \n",
      "1 -2.478130 -3.661264 -1.093923 -1.209320 -0.824395  \n",
      "2 -2.939352 -2.736450 -1.965399 -0.805868 -1.139434  \n",
      "3 -1.021929 -2.077843 -1.127629  0.331531 -2.179483  \n",
      "4 -2.566551 -1.675044 -1.082050 -0.965218 -1.836966  \n",
      "\n",
      "[5 rows x 2310 columns]\n",
      "(83, 2310)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../data')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# 83 tissue samples are classified into four cancer types based on 2308 gene expression measurements\n",
    "raw0 = pd.read_csv('Khan.csv') \n",
    "\n",
    "print(raw0.head())\n",
    "print(raw0.shape) # high-dimensional data (large # of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Select a kernel function and tune the penalty parameter \"C\" using CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'> i) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw0.iloc[:,2:]\n",
    "Y = raw0.iloc[:,1]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # suppress warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'> ii) Select a kernel function and tune the penalty parameter \"C\" using \"GridSearchCV\"\n",
    "* SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "* Available kernel functions: https://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "* Precision & Recall: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.983 (+/-0.067) for {'C': 1, 'kernel': 'rbf'}\n",
      "0.990 (+/-0.040) for {'C': 10, 'kernel': 'rbf'}\n",
      "0.990 (+/-0.040) for {'C': 100, 'kernel': 'rbf'}\n",
      "0.990 (+/-0.040) for {'C': 1000, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.855 (+/-0.230) for {'C': 1, 'degree': 5, 'kernel': 'poly'}\n",
      "0.744 (+/-0.158) for {'C': 1, 'degree': 10, 'kernel': 'poly'}\n",
      "0.634 (+/-0.180) for {'C': 1, 'degree': 15, 'kernel': 'poly'}\n",
      "0.914 (+/-0.139) for {'C': 10, 'degree': 5, 'kernel': 'poly'}\n",
      "0.819 (+/-0.156) for {'C': 10, 'degree': 10, 'kernel': 'poly'}\n",
      "0.627 (+/-0.174) for {'C': 10, 'degree': 15, 'kernel': 'poly'}\n",
      "0.914 (+/-0.139) for {'C': 100, 'degree': 5, 'kernel': 'poly'}\n",
      "0.819 (+/-0.156) for {'C': 100, 'degree': 10, 'kernel': 'poly'}\n",
      "0.627 (+/-0.174) for {'C': 100, 'degree': 15, 'kernel': 'poly'}\n",
      "0.914 (+/-0.139) for {'C': 1000, 'degree': 5, 'kernel': 'poly'}\n",
      "0.819 (+/-0.156) for {'C': 1000, 'degree': 10, 'kernel': 'poly'}\n",
      "0.627 (+/-0.174) for {'C': 1000, 'degree': 15, 'kernel': 'poly'}\n",
      "0.148 (+/-0.228) for {'C': 1, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.095 (+/-0.030) for {'C': 1, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.095 (+/-0.030) for {'C': 1, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "0.975 (+/-0.061) for {'C': 10, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.705 (+/-0.180) for {'C': 10, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.095 (+/-0.030) for {'C': 10, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "0.653 (+/-0.211) for {'C': 100, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.847 (+/-0.205) for {'C': 100, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.817 (+/-0.287) for {'C': 100, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "0.623 (+/-0.297) for {'C': 1000, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.653 (+/-0.254) for {'C': 1000, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.749 (+/-0.212) for {'C': 1000, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The scores are computed on test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.958 (+/-0.167) for {'C': 1, 'kernel': 'rbf'}\n",
      "0.983 (+/-0.067) for {'C': 10, 'kernel': 'rbf'}\n",
      "0.983 (+/-0.067) for {'C': 100, 'kernel': 'rbf'}\n",
      "0.983 (+/-0.067) for {'C': 1000, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.852 (+/-0.175) for {'C': 1, 'degree': 5, 'kernel': 'poly'}\n",
      "0.627 (+/-0.211) for {'C': 1, 'degree': 10, 'kernel': 'poly'}\n",
      "0.540 (+/-0.265) for {'C': 1, 'degree': 15, 'kernel': 'poly'}\n",
      "0.873 (+/-0.172) for {'C': 10, 'degree': 5, 'kernel': 'poly'}\n",
      "0.686 (+/-0.202) for {'C': 10, 'degree': 10, 'kernel': 'poly'}\n",
      "0.532 (+/-0.265) for {'C': 10, 'degree': 15, 'kernel': 'poly'}\n",
      "0.873 (+/-0.172) for {'C': 100, 'degree': 5, 'kernel': 'poly'}\n",
      "0.686 (+/-0.202) for {'C': 100, 'degree': 10, 'kernel': 'poly'}\n",
      "0.532 (+/-0.265) for {'C': 100, 'degree': 15, 'kernel': 'poly'}\n",
      "0.873 (+/-0.172) for {'C': 1000, 'degree': 5, 'kernel': 'poly'}\n",
      "0.686 (+/-0.202) for {'C': 1000, 'degree': 10, 'kernel': 'poly'}\n",
      "0.532 (+/-0.265) for {'C': 1000, 'degree': 15, 'kernel': 'poly'}\n",
      "0.275 (+/-0.100) for {'C': 1, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.250 (+/-0.000) for {'C': 1, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.250 (+/-0.000) for {'C': 1, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "0.963 (+/-0.100) for {'C': 10, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.712 (+/-0.159) for {'C': 10, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.250 (+/-0.000) for {'C': 10, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "0.641 (+/-0.147) for {'C': 100, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.840 (+/-0.219) for {'C': 100, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.826 (+/-0.232) for {'C': 100, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "0.539 (+/-0.144) for {'C': 1000, 'coef0': 0, 'kernel': 'sigmoid'}\n",
      "0.623 (+/-0.151) for {'C': 1000, 'coef0': 1, 'kernel': 'sigmoid'}\n",
      "0.731 (+/-0.201) for {'C': 1000, 'coef0': 2, 'kernel': 'sigmoid'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The scores are computed on test set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC # support vector machines for classification (SVR is for regression)\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree': [5, 10, 15], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['sigmoid'], 'coef0': [0, 1, 2], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='%s_macro' % score).fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The scores are computed on test set.\")\n",
    "    print()\n",
    "    print(classification_report(y_test, clf.predict(X_test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> Classification of Web Documents Using Naive Bayes\n",
    "* https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'> i) Import raw data (texts and their categories)\n",
    "* 20 news group data : \n",
    "    - http://qwone.com/~jason/20Newsgroups/\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# fetch_20newsgroups is a function !\n",
    "\n",
    "categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space'] # the entire data contains 20 categories but we'll be using only those categories\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes') # remove non-main text\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove, shuffle=True, random_state=42)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove, shuffle=True, random_state=42)\n",
    "\n",
    "# extract Y and X from the datasets\n",
    "Y_train = data_train.target \n",
    "Y_test = data_test.target\n",
    "\n",
    "X_train = data_train.data \n",
    "X_test = data_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how each category is indexed\n",
    "data_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "[1 3 2 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0]) # text\n",
    "print(Y_train) # integers (0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'> ii) Covert texts (bags of words) to numerical vectors\n",
    "* TfidfVectorizer \n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "    \n",
    "* Alternatively,\n",
    "    - CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "    - HashingVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer\n",
    "    \n",
    "* Stop words: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26576)\n",
      "(1353, 26576)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "Vectorizer=TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train = Vectorizer.fit_transform(X_train) \n",
    "X_test = Vectorizer.transform(X_test) \n",
    "\n",
    "# !!!Caution: Use \".fit_transform()\" for training data, but use \".transform()\" for testing data\n",
    "# This is to make sure the training and test sets have the same number of columns (features) \n",
    "# Here we are using the vectorizer trained for the training data to convert the testing data\n",
    "\n",
    "# check the size of X_train\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21025)\t0.14589314130201253\n",
      "  (0, 3998)\t0.07036762966055367\n",
      "  (0, 5546)\t0.14589314130201253\n",
      "  (0, 10605)\t0.16717988448915075\n",
      "  (0, 20973)\t0.09485258934884024\n",
      "  (0, 19841)\t0.06346990252225734\n",
      "  (0, 2408)\t0.0740617740444856\n",
      "  (0, 14706)\t0.04664380384555488\n",
      "  (0, 20977)\t0.09029017643192268\n",
      "  (0, 23828)\t0.2258173261964949\n",
      "  (0, 21208)\t0.10761272733713331\n",
      "  (0, 15301)\t0.10649668941393081\n",
      "  (0, 21084)\t0.06206087717654851\n",
      "  (0, 9848)\t0.10649668941393081\n",
      "  (0, 22878)\t0.11143515786946494\n",
      "  (0, 13023)\t0.13924444936699948\n",
      "  (0, 14154)\t0.04763446792799959\n",
      "  (0, 8554)\t0.10162931867025875\n",
      "  (0, 18949)\t0.13313300331371947\n",
      "  (0, 18704)\t0.10260670288726481\n",
      "  (0, 19066)\t0.4152868172245007\n",
      "  (0, 17464)\t0.22287031573892988\n",
      "  (0, 18699)\t0.0823823626717928\n",
      "  (0, 7698)\t0.11451045447542964\n",
      "  (0, 11203)\t0.07006583621645067\n",
      "  (0, 20513)\t0.14589314130201253\n",
      "  (0, 20239)\t0.13842893907483358\n",
      "  (0, 10286)\t0.44662488427974206\n",
      "  (0, 1152)\t0.36111859597627916\n",
      "  (0, 5385)\t0.10649668941393081\n",
      "  (0, 18701)\t0.13842893907483358\n",
      "  (0, 18440)\t0.11290866309824744\n",
      "  (0, 15322)\t0.1054444608710685\n",
      "  (0, 16089)\t0.08857647596970439\n",
      "  (0, 21204)\t0.18242163530798586\n",
      "  (0, 16965)\t0.10761272733713331\n",
      "  (0, 25319)\t0.059445923580199275\n",
      "  (0, 12052)\t0.07775591842841754\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'> iii) Run NB\n",
    "* https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78640059127864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       319\n",
      "           1       0.92      0.89      0.90       389\n",
      "           2       0.80      0.90      0.85       394\n",
      "           3       0.68      0.61      0.64       251\n",
      "\n",
      "    accuracy                           0.79      1353\n",
      "   macro avg       0.77      0.76      0.77      1353\n",
      "weighted avg       0.78      0.79      0.78      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "NBres= NB(alpha=.01).fit(X_train, Y_train) # alpha is a kind of a shrinkage parameter\n",
    "\n",
    "print(NBres.score(X_test, Y_test))\n",
    "print(classification_report(Y_test, NBres.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7634885439763488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.57      0.62       319\n",
      "           1       0.90      0.88      0.89       389\n",
      "           2       0.75      0.89      0.82       394\n",
      "           3       0.66      0.62      0.64       251\n",
      "\n",
      "    accuracy                           0.76      1353\n",
      "   macro avg       0.75      0.74      0.74      1353\n",
      "weighted avg       0.76      0.76      0.76      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SVC on the same data\n",
    "from sklearn.svm import SVC\n",
    "SVCres= SVC(kernel = 'linear', C = 10).fit(X_train, Y_train)\n",
    "\n",
    "print(SVCres.score(X_test, Y_test))\n",
    "print(classification_report(Y_test, SVCres.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkred'> HW7: Similarly to HW6-2, optimize SVC and NB on the newsgroups data\n",
    "\n",
    "* Select ten categories and import raw data under your categories. Follow the steps above to prepare datasets to run SVC and NB\n",
    "* Use the function \"GridSearchCV\" to optimize SVC and NB\n",
    "    - To optimize SVC, select a kernel function and tune \"C\" parameter\n",
    "    - To optimize NB, tune \"alpha\" parameter\n",
    "* Use both \"precision\" and \"recall\" to evaluate prediction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "        'alt.atheism',\n",
    "'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    "'rec.sport.baseball'] \n",
    "\n",
    "remove = ('headers', 'footers', 'quotes') # remove non-main text\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove, shuffle=True, random_state=10)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove, shuffle=True, random_state=10)\n",
    "\n",
    "# extract Y and X from the datasets\n",
    "Y_train = data_train.target \n",
    "Y_test = data_test.target\n",
    "\n",
    "X_train = data_train.data \n",
    "X_test = data_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target names length:  10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how each category is indexed\n",
    "print('Target names length: ', len(data_train.target_names))\n",
    "data_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5790, 67164)\n",
      "(3855, 67164)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "Vectorizer=TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train = Vectorizer.fit_transform(X_train) \n",
    "X_test = Vectorizer.transform(X_test) \n",
    "\n",
    "# !!!Caution: Use \".fit_transform()\" for training data, but use \".transform()\" for testing data\n",
    "# This is to make sure the training and test sets have the same number of columns (features) \n",
    "# Here we are using the vectorizer trained for the training data to convert the testing data\n",
    "\n",
    "# check the size of X_train\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GridSearchCV on SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['precision', 'recall']\n",
    "\n",
    "params = {'kernel': ['linear'], 'C': [1, 10, 100]}\n",
    "\n",
    "for score in scores:\n",
    "    clf_svc = GridSearchCV(SVC(), params, cv=3, scoring='%s_macro' % score).fit(X_train, Y_train)\n",
    "    print(f'SVC Best params ({score}): ', clf_svc.best_params_, '\\n')\n",
    "    means = clf_svc.cv_results_['mean_test_score']\n",
    "    for mean, params in zip(means, clf_svc.cv_results_['params']):\n",
    "        print(\"%0.3f for %r \\n\" % (mean, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GridSearchCV on NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['precision', 'recall']\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "for score in scores:\n",
    "    clf_nb = GridSearchCV(NB(), params, cv=5, scoring='%s_macro' % score).fit(X_train, Y_train)\n",
    "    print(f'NB Best params ({score}): ', clf_svc.best_params_, '\\n')\n",
    "    means = clf_nb.cv_results_['mean_test_score']\n",
    "    for mean, params in zip(means, clf_nb.cv_results_['params']):\n",
    "        print(\"%0.3f for %r \\n\" % (mean, params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
